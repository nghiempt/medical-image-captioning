# DPL303m - Medical Image Captioning

## Introduction

Medical Image Captioning (MIC) is a challenging task in the realm of artificial intelligence, aimed at automatically generating descriptive and accurate textual descriptions for medical images. This process involves several mandatory techniques to ensure robust performance and meaningful outputs.


## Software implementation

> Briefly describe the software that was written to produce the results of this
> paper.

All source code used to generate the results and figures in the paper are in
the `code` folder.
The calculations and figure generation are all run inside
[Jupyter notebooks](http://jupyter.org/).
The data used in this study is provided in `data` and the sources for the
manuscript text and figures are in `manuscript`.
Results generated by the code are saved in `results`.
See the `README.md` files in each directory for a full description.


## Getting the code

You can download a copy of all the files in this repository by cloning the
[git](https://git-scm.com/) repository:

    git clone https://github.com/nghiempt/medical-image-captioning

or [download a zip archive](https://github.com/nghiempt/medical-image-captioning/archive/refs/heads/main.zip).

A copy of the repository is also archived at *insert DOI here*


## Approach

. . .

## Evaluation

1. **BLEU**

   > Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation. In *Proceedings of meeting of the association for computational linguistics* (pp. 311–318). 

   BLEU has frequently been reported as correlating well with human judgement, and remains a benchmark for the assessment of any new evaluation metric. There are however a number of criticisms that have been voiced. It has been noted that, although in principle capable of evaluating translations of any language, BLEU cannot, in its present form, deal with languages lacking word boundaries. It has been argued that although BLEU has significant advantages, there is no guarantee that an increase in BLEU score is an indicator of improved translation quality.

2. **ROUGE-L**

   > Lin, C.-Y. (2004). ROUGE: A package for automatic evaluation of summaries. In *Proceedings of meeting of the association for computational linguistics* (pp. 74–81). 

   ROUGE-L: Longest Common Subsequence (LCS) based statistics. [Longest common subsequence problem](https://en.wikipedia.org/wiki/Longest_common_subsequence_problem) takes into account sentence level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.


## Dataset detail
| id            | appN           | pkgN | iCr          | iCm         | iCs         |
| ------------- | -------------  | ------------ | ------------ | -----------  |-----------  |
| 1             | Once: Perfect Match Dating App | com.udates | 1 | 0 | 0 |
| 2             | Photo Collage Maker Editor | cornera.touchretouch | 0 | 1 | 0 |
| 3             | Pict2Cam | com.adriangl.pict2cam | 1 | 0 | 0 |
| 4             | Blend Photo Editor – Effects | com.multiphotoblender.photomixer | 1 | 0 | 0 |
| 5             | Wild Animal Photo Frames | com.appbites.wildanimalphotoframes | 1 | 0 | 0 |
| 6             | Baby Photo Nice Baby wallpaper | com.cuteBaby.BabyPhotos | 1 | 0 | 0 |
| 7             | Mobile Phone Photo Frames | freeappshouse.mobile.phone.photo.frames.editor | 1 | 0 | 0 |
| 8             | CSL – Meet, Chat, Pla‪y & Date | com.jaumo.casual | 1 | 0 | 0 |
| 9             | Body Plastic Surgery | com.ster.photo.surgery | 1 | 0 | 0 |
| 10            | Family Photo Frame | com.Family.Photoframee | 1 | 0 | 0 |

Full dataset can be found in [kaggle](https://www.kaggle.com/datasets/nghiemthanhpham/inbreast).