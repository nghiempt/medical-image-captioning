{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Add\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from underthesea import word_tokenize\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import lime\n",
    "import lime.lime_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_train(base_path='../dataset/train'):\n",
    "    image_paths = []\n",
    "    captions = []\n",
    "    for img_name in os.listdir(f'{base_path}/images'):\n",
    "        if img_name.endswith('.jpg'):\n",
    "            image_path = f'{base_path}/images/{img_name}'\n",
    "            caption_path = f'{base_path}/captions/{img_name.replace(\".jpg\", \".txt\")}'\n",
    "\n",
    "            with open(caption_path, 'r') as f:\n",
    "                caption = f.read()\n",
    "\n",
    "            # Tokenize Vietnamese captions\n",
    "            caption_tokens = word_tokenize(caption, format=\"text\")\n",
    "            captions.append(caption_tokens)\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "    return image_paths, captions\n",
    "\n",
    "image_paths, captions = load_dataset_train()\n",
    "print(f'Loaded {len(image_paths)} images and {len(captions)} captions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(captions)\n",
    "max_seq_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "print(f'Vocab size: {vocab_size}, Max sequence length: {max_seq_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3), pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from images\n",
    "def extract_features(image_paths):\n",
    "    image_features = []\n",
    "    for img_path in image_paths:\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = tf.keras.applications.resnet50.preprocess_input(img)\n",
    "        features = resnet.predict(img)\n",
    "        image_features.append(features)\n",
    "    return np.array(image_features)\n",
    "\n",
    "image_features = extract_features(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "inputs2 = Input(shape=(max_seq_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = Add()([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to integers using Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(captions)\n",
    "\n",
    "# Generate sequences of input-output pairs for training\n",
    "def create_sequences(sequences, max_seq_length, vocab_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for seq in sequences:\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_seq_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            X1.append(image_features[i])  # Image feature for the current word\n",
    "            X2.append(in_seq)  # Sequence up to the current word\n",
    "            y.append(out_seq)  # The next word\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "# Create input-output sequences\n",
    "X1, X2, y = create_sequences(sequences, max_seq_length, vocab_size)\n",
    "\n",
    "X1 = np.squeeze(X1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X1 shape:\", X1.shape)\n",
    "print(\"X2 shape:\", X2.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset(base_path='../dataset_test'):\n",
    "    test_image_paths = []\n",
    "    test_captions = []\n",
    "    for img_name in os.listdir(f'{base_path}/images'):\n",
    "        if img_name.endswith('.jpg'):\n",
    "            image_path = f'{base_path}/images/{img_name}'\n",
    "            caption_path = f'{base_path}/captions/{img_name.replace(\".jpg\", \".txt\")}'\n",
    "\n",
    "            with open(caption_path, 'r') as f:\n",
    "                caption = f.read()\n",
    "\n",
    "            test_image_paths.append(image_path)\n",
    "            test_captions.append(caption)\n",
    "\n",
    "    return test_image_paths, test_captions\n",
    "\n",
    "test_image_paths, test_captions = load_test_dataset()\n",
    "print(f'Loaded {len(test_image_paths)} images and {len(test_captions)} captions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_paths, test_captions = load_test_dataset()\n",
    "\n",
    "# Function to map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# Function to generate caption for a given image\n",
    "def generate_caption(model, photo, tokenizer, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class BLEUEvaluator(Callback):\n",
    "    def __init__(self, test_image_paths, test_captions, tokenizer, max_seq_length):\n",
    "        self.test_image_paths = test_image_paths\n",
    "        self.test_captions = test_captions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        predicted_captions = []\n",
    "        for i in range(len(self.test_image_paths)):\n",
    "            photo = image_features[i].reshape((1, 2048))\n",
    "            caption = generate_caption(model, photo, tokenizer, self.max_seq_length)\n",
    "            predicted_captions.append(caption)\n",
    "\n",
    "        references = [[caption.split()] for caption in self.test_captions]\n",
    "        hypotheses = [caption.split() for caption in predicted_captions]\n",
    "\n",
    "        bleu_score = corpus_bleu(references, hypotheses)\n",
    "        print(\"BLEU Score after epoch {}: {}\".format(epoch+1, bleu_score))\n",
    "\n",
    "bleu_evaluator = BLEUEvaluator(test_image_paths, test_captions, tokenizer, max_seq_length)\n",
    "\n",
    "model.fit([X1, X2], y, epochs=2, batch_size=32, callbacks=[bleu_evaluator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/Users/nghiempt/Downloads/DPL303m_FINAL/dataset_test/images/20586908_6c613a14b80a8591_MG_R_CC_ANON.jpg'\n",
    "image = Image.open(image_path)\n",
    "image_data = np.array(image)\n",
    "\n",
    "explainer = lime.lime_image.LimeImageExplainer()\n",
    "\n",
    "def predict_fn(images):\n",
    "    return model.predict(images)\n",
    "\n",
    "explanation = explainer.explain_instance(image_data, predict_fn, top_labels=5, hide_color=0, num_samples=1000)\n",
    "\n",
    "# Show the explanation\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
    "img_boundry1 = mark_boundaries(temp / 2 + 0.5, mask)\n",
    "plt.imshow(img_boundry1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
