{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting lime\n",
      "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from lime) (3.7.5)\n",
      "Requirement already satisfied: numpy in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from lime) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from lime) (1.10.1)\n",
      "Requirement already satisfied: tqdm in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from lime) (4.66.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from lime) (1.3.2)\n",
      "Collecting scikit-image>=0.12 (from lime)\n",
      "  Downloading scikit_image-0.21.0-cp38-cp38-macosx_10_9_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: networkx>=2.8 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from scikit-image>=0.12->lime) (3.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from scikit-image>=0.12->lime) (10.2.0)\n",
      "Collecting imageio>=2.27 (from scikit-image>=0.12->lime)\n",
      "  Downloading imageio-2.34.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image>=0.12->lime)\n",
      "  Downloading tifffile-2023.7.10-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting PyWavelets>=1.1.1 (from scikit-image>=0.12->lime)\n",
      "  Downloading PyWavelets-1.4.1-cp38-cp38-macosx_10_13_x86_64.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: packaging>=21 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from scikit-image>=0.12->lime) (23.2)\n",
      "Collecting lazy_loader>=0.2 (from scikit-image>=0.12->lime)\n",
      "  Downloading lazy_loader-0.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from scikit-learn>=0.18->lime) (3.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from matplotlib->lime) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from matplotlib->lime) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from matplotlib->lime) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from matplotlib->lime) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from matplotlib->lime) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from matplotlib->lime) (6.1.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/nghiempt/Library/Python/3.8/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib->lime) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.15.0)\n",
      "Downloading scikit_image-0.21.0-cp38-cp38-macosx_10_9_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.34.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.4/313.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Downloading PyWavelets-1.4.1-cp38-cp38-macosx_10_13_x86_64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tifffile-2023.7.10-py3-none-any.whl (220 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lime\n",
      "  Building wheel for lime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283846 sha256=cae884718e458c1d3b6f8ea6e60fdc2cb7b65198c5b7ba461f3cbfddfc16c3a9\n",
      "  Stored in directory: /Users/nghiempt/Library/Caches/pip/wheels/e6/a6/20/cc1e293fcdb67ede666fed293cb895395e7ecceb4467779546\n",
      "Successfully built lime\n",
      "Installing collected packages: tifffile, PyWavelets, lazy_loader, imageio, scikit-image, lime\n",
      "Successfully installed PyWavelets-1.4.1 imageio-2.34.0 lazy_loader-0.3 lime-0.2.0.1 scikit-image-0.21.0 tifffile-2023.7.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 2048)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 128, 256)             70912     ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 2048)                 0         ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 128, 256)             0         ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 256)                  524544    ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 256)                  525312    ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 256)                  0         ['dense_3[0][0]',             \n",
      "                                                                     'lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 256)                  65792     ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 277)                  71189     ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1257749 (4.80 MB)\n",
      "Trainable params: 1257749 (4.80 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=2048 must be between 0 and min(n_samples, n_features)=296 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m     image_data\u001b[38;5;241m.\u001b[39mappend(features\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m     89\u001b[0m image_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image_data)\n\u001b[0;32m---> 90\u001b[0m image_data_pca \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m image_features \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_paths):\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/decomposition/_pca.py:460\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    439\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/decomposition/_pca.py:510\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/decomposition/_pca.py:524\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    521\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is only supported if n_samples >= n_features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m         )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m must be between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_components, \u001b[38;5;28mmin\u001b[39m(n_samples, n_features))\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# Center data\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=2048 must be between 0 and min(n_samples, n_features)=296 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Dropout, add\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from underthesea import word_tokenize\n",
    "import shap\n",
    "from lime import lime_image\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def load_dataset(base_path='Inbreast'):\n",
    "    image_paths = []\n",
    "    captions = []\n",
    "    for img_name in os.listdir(f'{base_path}/image'):\n",
    "        if img_name.endswith('.jpg'):\n",
    "            image_path = f'{base_path}/image/{img_name}'\n",
    "            caption_path = f'{base_path}/caption/{img_name.replace(\".jpg\", \".txt\")}'\n",
    "\n",
    "            with open(caption_path, 'r') as f:\n",
    "                caption = f.read()\n",
    "\n",
    "            image_paths.append(image_path)\n",
    "            captions.append(caption)\n",
    "\n",
    "    return image_paths, captions\n",
    "\n",
    "image_paths, captions = load_dataset()\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array_expanded = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array_expanded)\n",
    "\n",
    "resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# Tokenization và Padding cho tiếng Việt\n",
    "tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
    "captions_tokenized = [word_tokenize(caption) for caption in captions]\n",
    "tokenizer.fit_on_texts(captions_tokenized)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(captions_tokenized)\n",
    "max_length = max(len(s) for s in sequences)\n",
    "captions_padded = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "def build_model(vocab_size, max_length):\n",
    "    # Image feature extractor layer\n",
    "    inputs1 = Input(shape=(2048,))  # ResNet50 output\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # Sequence processor layer\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # Decoder layer\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # Tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "model = build_model(vocab_size, max_length)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# PCA cho hình ảnh\n",
    "pca = PCA(n_components=2048)\n",
    "image_data = []\n",
    "for img_path in image_paths:\n",
    "    preprocessed_img = preprocess_image(img_path)\n",
    "    features = resnet.predict(preprocessed_img, verbose=0)\n",
    "    image_data.append(features.flatten())\n",
    "image_data = np.array(image_data)\n",
    "image_data_pca = pca.fit_transform(image_data)\n",
    "\n",
    "image_features = {}\n",
    "for i, img_path in enumerate(image_paths):\n",
    "    image_id = img_path.split('/')[-1].split('.')[0]\n",
    "    image_features[image_id] = image_data_pca[i]\n",
    "\n",
    "images = list(image_features.keys())\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def data_generator(captions, image_features, tokenizer, max_length, batch_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    while 1:\n",
    "        for i, caption in enumerate(captions):\n",
    "            n+=1\n",
    "            image_id = images[i]\n",
    "            photo = image_features[image_id]\n",
    "            seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                X1.append(photo)\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "\n",
    "            if n == batch_size:\n",
    "                yield [[np.array(X1), np.array(X2)], np.array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0\n",
    "\n",
    "batch_size = 1\n",
    "steps = len(captions) // batch_size\n",
    "\n",
    "# Sử dụng SHAP và LIME\n",
    "explainer_shap = shap.Explainer(model.predict, data=(image_data_pca, captions_padded))\n",
    "explainer_lime = lime_image.LimeImageExplainer()\n",
    "\n",
    "for i in range(5):\n",
    "    generator = data_generator(captions, image_features, tokenizer, max_length, batch_size)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\n",
    "    # Sử dụng SHAP và LIME\n",
    "    shap_values = explainer_shap.shap_values((specific_image_pca, specific_caption))\n",
    "    explanation_lime = explainer_lime.explain_instance(specific_image, model.predict)\n",
    "    shap.image_plot(shap_values[0], specific_image_pca)\n",
    "    explanation_lime.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_paths_train, image_paths_test, captions_train, captions_test = train_test_split(image_paths, captions_padded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tạo generator cho tập huấn luyện và tập kiểm tra\n",
    "batch_size = 32\n",
    "train_steps = len(captions_train) // batch_size\n",
    "test_steps = len(captions_test) // batch_size\n",
    "\n",
    "train_generator = data_generator(captions_train, image_features, tokenizer, max_length, batch_size)\n",
    "test_generator = data_generator(captions_test, image_features, tokenizer, max_length, batch_size)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "epochs = 10\n",
    "history = model.fit(train_generator, epochs=epochs, steps_per_epoch=train_steps, validation_data=test_generator, validation_steps=test_steps)\n",
    "\n",
    "# Đánh giá mô hình trên tập kiểm tra\n",
    "loss, accuracy = model.evaluate(test_generator, steps=test_steps)\n",
    "print(\"Loss on test set:\", loss)\n",
    "print(\"Accuracy on test set:\", accuracy)\n",
    "\n",
    "# Vẽ biểu đồ loss và accuracy\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
