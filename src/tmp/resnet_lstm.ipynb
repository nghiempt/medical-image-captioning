{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Dropout, add\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from underthesea import word_tokenize\n",
    "import shap\n",
    "from lime import lime_image\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 296 images and 296 captions\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(base_path='../dataset'):\n",
    "    image_paths = []\n",
    "    captions = []\n",
    "    for img_name in os.listdir(f'{base_path}/images'):\n",
    "        if img_name.endswith('.jpg'):\n",
    "            image_path = f'{base_path}/images/{img_name}'\n",
    "            caption_path = f'{base_path}/captions/{img_name.replace(\".jpg\", \".txt\")}'\n",
    "\n",
    "            with open(caption_path, 'r') as f:\n",
    "                caption = f.read()\n",
    "\n",
    "            image_paths.append(image_path)\n",
    "            captions.append(caption)\n",
    "\n",
    "    return image_paths, captions\n",
    "\n",
    "image_paths, captions = load_dataset()\n",
    "print(f'Loaded {len(image_paths)} images and {len(captions)} captions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array_expanded = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array_expanded)\n",
    "\n",
    "resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "# PCA cho hình ảnh\n",
    "pca = PCA(n_components=128) \n",
    "\n",
    "image_data = []\n",
    "image_features = {}\n",
    "\n",
    "for img_path in image_paths:\n",
    "    preprocessed_img = preprocess_image(img_path)\n",
    "    features = resnet.predict(preprocessed_img, verbose=0)\n",
    "    image_data.append(features.flatten())\n",
    "\n",
    "image_data = np.array(image_data)\n",
    "image_data_pca = pca.fit_transform(image_data)\n",
    "\n",
    "for i, img_path in enumerate(image_paths):\n",
    "    image_id = img_path.split('/')[-1].split('.')[0]\n",
    "    image_features[image_id] = image_data_pca[i]\n",
    "\n",
    "images = list(image_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 277, Max length: 128\n",
      "Original: Nhũ ảnh ghi lại sự biến dạng của mô đệm của vú trái, có đường kính 15 mm, tổn thương có tiêu chí nghi ngờ trung gian.  Kết quả hình ảnh lành tính BiRads 2.\n",
      "Tokenized: ['Nhũ', 'ảnh', 'ghi', 'lại', 'sự', 'biến dạng', 'của', 'mô đệm', 'của', 'vú', 'trái', ',', 'có', 'đường kính', '15', 'mm', ',', 'tổn thương', 'có', 'tiêu chí', 'nghi ngờ', 'trung gian', '.', 'Kết quả', 'hình ảnh', 'lành tính', 'BiRads', '2', '.']\n",
      "Padded: [ 19  20 115 101  57 159  43 160  43   5  40   3   4  49 126  37   3  61\n",
      "   4 127  30 145   2  18   6  12   7  17   2   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "\n",
      "Original: Vôi hóa tổng thể lành tính được quan sát thấy ở cả hai bên. Không có hình ảnh nốt nào gợi ý ác tính, vôi hóa mô tuyến vú đáng ngờ hoặc những thay đổi đáng kể khác được xác định ở cả hai bên. Kết quả hình ảnh lành tính BiRads 2.\n",
      "Tokenized: ['Vôi', 'hóa', 'tổng thể', 'lành tính', 'được', 'quan sát', 'thấy', 'ở', 'cả', 'hai', 'bên', '.', 'Không', 'có', 'hình ảnh', 'nốt', 'nào', 'gợi ý', 'ác tính', ',', 'vôi', 'hóa', 'mô', 'tuyến', 'vú', 'đáng', 'ngờ', 'hoặc', 'những', 'thay đổi', 'đáng kể', 'khác', 'được', 'xác định', 'ở', 'cả', 'hai', 'bên', '.', 'Kết quả', 'hình ảnh', 'lành tính', 'BiRads', '2', '.']\n",
      "Padded: [ 29  14 161  12  25  63  53   8  24  15  22   2  11   4   6  10  27  21\n",
      "   9   3  29  14  54  13   5  23  26  34  28  31  36  33  25  45   8  24\n",
      "  15  22   2  18   6  12   7  17   2   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "\n",
      "Original: Vôi hóa tổng thể lành tính được quan sát thấy ở cả hai bên. Không có hình ảnh nốt nào gợi ý ác tính, vôi hóa mô tuyến vú đáng ngờ hoặc những thay đổi đáng kể khác được xác định ở cả hai bên. Kết quả hình ảnh lành tính BiRads 2.\n",
      "Tokenized: ['Vôi', 'hóa', 'tổng thể', 'lành tính', 'được', 'quan sát', 'thấy', 'ở', 'cả', 'hai', 'bên', '.', 'Không', 'có', 'hình ảnh', 'nốt', 'nào', 'gợi ý', 'ác tính', ',', 'vôi', 'hóa', 'mô', 'tuyến', 'vú', 'đáng', 'ngờ', 'hoặc', 'những', 'thay đổi', 'đáng kể', 'khác', 'được', 'xác định', 'ở', 'cả', 'hai', 'bên', '.', 'Kết quả', 'hình ảnh', 'lành tính', 'BiRads', '2', '.']\n",
      "Padded: [ 29  14 161  12  25  63  53   8  24  15  22   2  11   4   6  10  27  21\n",
      "   9   3  29  14  54  13   5  23  26  34  28  31  36  33  25  45   8  24\n",
      "  15  22   2  18   6  12   7  17   2   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization và Padding cho tiếng Việt\n",
    "tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
    "captions_tokenized = [word_tokenize(caption) for caption in captions]\n",
    "tokenizer.fit_on_texts(captions_tokenized)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(captions_tokenized)\n",
    "max_length = max(len(s) for s in sequences)\n",
    "captions_padded = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "print(f'Vocab size: {vocab_size}, Max length: {max_length}')\n",
    "# print samples\n",
    "for i in range(3):\n",
    "    print(f'Original: {captions[i]}')\n",
    "    print(f'Tokenized: {captions_tokenized[i]}')\n",
    "    print(f'Padded: {captions_padded[i]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)        [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, 128, 256)             70912     ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 128)                  0         ['input_8[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 128, 256)             0         ['embedding_3[0][0]']         \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 256)                  33024     ['dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               (None, 256)                  525312    ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 256)                  0         ['dense_9[0][0]',             \n",
      "                                                                     'lstm_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 256)                  65792     ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 277)                  71189     ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 766229 (2.92 MB)\n",
      "Trainable params: 766229 (2.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, max_length):\n",
    "    # Image feature extractor layer\n",
    "    inputs1 = Input(shape=(128,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # Sequence processor layer\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # Decoder layer\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)  # Update output dimension\n",
    "\n",
    "    # Tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "model = build_model(vocab_size, max_length)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(captions, image_features, tokenizer, max_length, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(captions), batch_size):\n",
    "            X1_batch = []\n",
    "            X2_batch = []\n",
    "            y_batch = []\n",
    "\n",
    "            for j in range(i, min(i + batch_size, len(captions))):\n",
    "                caption = captions[j]\n",
    "                image_id = images[j]\n",
    "                photo = image_features[image_id]\n",
    "\n",
    "                # Convert caption to string if it's a numpy array\n",
    "                if isinstance(caption, np.ndarray):\n",
    "                    caption = ' '.join([tokenizer.index_word[idx] for idx in caption if idx != 0])\n",
    "\n",
    "                # Tokenize caption\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "                for k in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:k], seq[k]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                    X1_batch.append(photo)\n",
    "                    X2_batch.append(in_seq)\n",
    "                    y_batch.append(out_seq)\n",
    "\n",
    "            yield ([np.array(X1_batch), np.array(X2_batch)], np.array(y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set: 236 images and 236 captions\n",
      "Size of test set: 60 images and 60 captions\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Chia dataset thành tập huấn luyện và tập kiểm tra\n",
    "X_train_img, X_test_img, X_train_seq, X_test_seq, y_train, y_test = train_test_split(image_data_pca, captions_padded, captions_padded, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Size of train set: {len(X_train_img)} images and {len(X_train_seq)} captions')\n",
    "print(f'Size of test set: {len(X_test_img)} images and {len(X_test_seq)} captions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7/7 [==============================] - ETA: 0s - loss: 4.0560"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/engine/training.py\", line 1972, in test_function  *\n        return step_function(self, iterator)\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/engine/training.py\", line 1956, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/engine/training.py\", line 1944, in run_step  **\n        outputs = model.test_step(data)\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/engine/training.py\", line 1852, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (60, 128) and (60, 277) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Huấn luyện mô hình\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_test_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_seq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/81/p37gc41n4ds1pfphccjpd46w0000gn/T/__autograph_generated_file6ywx82cl.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/engine/training.py\", line 1972, in test_function  *\n        return step_function(self, iterator)\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/engine/training.py\", line 1956, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/engine/training.py\", line 1944, in run_step  **\n        outputs = model.test_step(data)\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/engine/training.py\", line 1852, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/nghiempt/Library/Python/3.8/lib/python/site-packages/keras/src/backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (60, 128) and (60, 277) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Thiết lập tham số\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model.fit(\n",
    "    data_generator(X_train_seq, image_features, tokenizer, max_length, batch_size),\n",
    "    steps_per_epoch=len(X_train_seq)//batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=([X_test_img, X_test_seq], y_test),\n",
    "    validation_steps=len(X_test_seq)//batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
